# Llama 2: Open Foundation and Fine-Tuned Chat Models

GenAI, Meta
논문 링크 : [10000000_662098952474184_2584067087619170692_n.pdf (fbcdn.net)
](https://scontent-ssn1-1.xx.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TMGkGNuGeJcAX-IQZyB&_nc_ht=scontent-ssn1-1.xx&oh=00_AfBWdDl5kB7NoCb4TrVIJAj7dUna5qqQcAFs3zM2tYSQZA&oe=64C25B7F)

## Abstract
본 연구에서 7B에서 70B의 매개변수를 가진 대형 언어모델 Llama2를 개발하고 출시했음을 알려주고 있습니다. Llama2-Chat이라고 불리는 미세조정된 LLM은 대화에 최적화 되어있으며, 폐쇄적인 소스를 기반으로 제작된 모델을 대체하는 것에 적합할 수 있다 말합니다. LLM의 책임있는 개발에 기여할 수 있도록 Llama2-Chat의 미세조정 및 접근 방식에 대한 자세한 설명을 제공할 것입니다.


## Introduction
LLM은 프로그램 및 창의적 글쓰기와 같은 전문분야는 물론 광범위한 분야에 걸쳐 큰 가능성을 보여주고 있습니다. 본 연구에서는 사전 훈련되고 미세조정된 LLM 제품군인 Llama2와 Llama2-Chat을 최대 70B 매개변수로 개발 및 릴리스합니다. 폐쇄형 LLM이 아니라 연구 및 상업적 사용이 가능한 모델 아래 두가지를 공개하였습니다.

1. Llama2 : 7B 13B 70B의 매개변수를 가진 모델로, 34B도 훈련하였으나 출시는 하지 않았습니다.
2. Llama2-Chat : 7B 130B 70B 매개변수를 가진 모델로 사용자와의 채팅에 최적화되도록 fine tuning한 모델입니다.


## Llama2-Chat의 training 방법
![1](https://github.com/semi0612/paper/assets/51469989/9c1b71c8-c821-444e-8754-a70cb0870f5e)

공개적으로 사용이 가능한 소스를 활용하여 Pretraining을 한 후, Fine Tuning을 사용하여 Chat의 초기버젼을 제작합니다. 모델은 거부 샘플링(rejection sampling)과 PPO(근접 정책 최적화)를 통해 RLHF(인간 피드백을 통한 강화학습) 하며 반복적으로 세분화됩니다. 이때 RLHF 단계에서는 반복적인 보상 모델링을 데이터를 축적하여 보상 모델이 잘 분포되도록 유지해야 하는데, 이는 모델 향상과 병행되는 중요한 부분입니다.

* Rrejection sampling(거부 샘플링)
주어진 확률분포에서 효율적으로 샘플을 생성하기 위해 많이 이용되는 알고리즘. 임의의 수식을 가진 확률 밀도 함수로부터 랜덤한 샘플 추출시 사용된다. 타켓 분포 안쪽에 존재하는 (기준 함수 선보다 확률이 낮은) 값들은 받아들여 샘플링하고, 그렇지 않으면 거절하는 방식
* PPO(Proximal Policy Optimization; 근접 정책 최적화)
Model-Free based learning의 한 강화학습 방식으로 Policy Gradient Learning(신경망을 이용하여 Softmax Regression 방식으로 확률을 구하고 분포를 얻어 학습하는 방식)의 단점을 보안한 모델. 학습데이터를 재사용하는 재사용하며, Episode 가 끝난 뒤 결과를 반영하는게 아니라 step 단위로 학습에 반영한다.

## Pretraining Data
공개적으로 사용 가능한 소스와 Meta의 제품 혹은 서비스에서 얻은 데이터를 활용한다. 단, 민감한 개인정보와 관련된 내용은 제외되어있다.


## Training Details
![2](https://github.com/semi0612/paper/assets/51469989/f58f5c73-a278-4ba4-a1a1-e51b165bf324)

Llama1의 사전 훈련 설정 및 모델 아키텍쳐를 그대로 사용하였다. 차이가 있다면 컨텍스트의 길이를 두 배로 증가시켰다는 점이다. 2T개의 토큰에 대한 Pretraining을 진행시켰고, 테스트 결과 과적합되지 않는 것을 확인하였습니다.

표준 변환 아키텍처(standard transformer architecture)를 사용하였고, RMSNorm로 사전정규화 적용, 활성화함수로는 SwiGLU, 옵티마이져로는 AdamW(β1 =0.9, β2 = 0.95, eps = 10−5) 이용하였습니다. 토크나이져로는 BPE(Bytepair Encoding) 알고리즘을 사용하여 (Llama1과 동일한 적용으로) 모든 숫자는 개별 숫자로 나누고, 알수없는 문자는 분해합니다.. 이렇게 만들어진 토큰의 전체 크기는 32K입니다. 하드웨어로는 NVIDIA A100을 이용하였습니다.

![3](https://github.com/semi0612/paper/assets/51469989/23203de5-ee88-4bf5-9a24-07e03e2112c5)


추가적으로 위와같이, 진행한 사전교육 중 CO2 배출 값도 표로 정리하여 보여주고 있다. 전력 사용을 하며 생성된 배출물은 전부 Meta의 지속 가능성 프로그램(Meta’s sustainability program)에 의해 상쇄된다고 한다.

## Llama2 Pretrained Model Evaluation
![4](https://github.com/semi0612/paper/assets/51469989/eacbd1a0-523d-44ea-88d6-e5aec46d38fd)

* MPT 모델 : Llama1 7B와 비슷한 품질을 가졌다고 평가되며, 상업적 사용이 가능한 오픈소스 모델

널리 사용되는 벤치마크들에 대해 전반적인 성능을 요약해놓은 표 입니다. Llama1과 Llama2를 비교해봤을 때, 확실히 더 우수한 점수를 내고 있는 것은 분명해 보입니다. 같은 크기인 7B로 비교한 점수차이가 아래와 같습니다.
```
		Llama1 : 14.1	60.8	46.2	58.5	6.95	35.1	30.3	23.9
		Llama2 : 16.8	63.9	48.9	61.3	14.6	45.3	32.6	29.3
```
또한 MPT와 Falcon 같은 오픈소스 모델과 비교해본 결과로는 Llama2가 훨씬 더 우수한 성능을 보인다는 걸 점수로 말해주고 있습니다. 특히나 수학과 MMLU 벤치마크 결과를 비교해보면 격차를 확실히 벌리고 있음을 알 수 있었습니다.

![5](https://github.com/semi0612/paper/assets/51469989/0bf9e467-ba31-4a9a-8c9e-bcee337c4959)

폐쇄적인 LLM인 GPT3.5 GPT4 PaLM PaLM-2-L 과도 (공개된 공식 Paper에서 가져온) 값을 비교하여 표로 보여주고 있습니다. 확실히 GPT-4가 거의 모든 벤치마크 비교에서 뛰어난 점수를 보여주고 있기는 하지만, GPT-3.5나 PaLM과는 비슷한 성능을 보이고 있음을 알려주고 있습니다.


## Fine Tuning
새로운 기술인 GAtt(Ghost Attention)을 사용했다고 하며, 이는 여러 턴에 걸쳐 대화의 흐름제어에 도움을 줬다고 합니다.

### 지도 미세 조정(Supervised Fine-Tuning; SFT)
공개적으로 사용 가능한 명령어 튜닝으로 SFT 단계를 시작한다. 타사 SFT 데이터의 다수는 대화식 LLM 지침에 맞추기에는 다양성과 품질이 부족하기 때문에 예시가 될 만한 수천가지의 고품질 SFT 데이터를 수집하는데 중점을 두고있습니다. 그렇게 자체 공급한 수만 개의 SFT 데이터는 더 높은 품질을 가지고 있기 때문에 고품질의 결과를 얻기에 충분하다는 것을 발견하였고, 총 27,540개의 주석을 수집한 후 데이터 추가 생성을 중지했습니다. 이때 사용자 메타 데이터는 포함하지 않았습니다. 데이터 품질을 검증하기 위해 인간이 제공한 주석과 수동 정밀 조사를 통해 모델이 생성한 샘플을 비교하며 180개의 예제 세트를 신중히 검토 했습니다. 그 결과 놀랍게도 SFT 모델에서 샘플링 된 출력이 인간이 손으로 작성한 SFT 주석과 종종 비슷하다는(경쟁한다는) 것을 발견하였고, 이는 RLHF 기반 주석에 우선순위를 지정하여 필요한 곳에 더 많은 다른 노력을 쏟게 할 수도 있다는 점을 시사했습니다.

![6](https://github.com/semi0612/paper/assets/51469989/71b3aa7c-0bf2-43ef-8f18-e26f56c0165a)


각 샘플은 [프롬프트와 응답]으로 구성됩니다. 설정값은 Learning Rate는 2 × 10-5, weight decay는 0.1, bacth size는 64, sequence length는 4096, 2 epoch. 모델의 시퀸스 길이가 적절히 채워질 수 있게 Training set의 모든 prompt와 답변을 연결하고, 프롬프트와 응답 세그먼트를 구분하기 위해 특수 토큰이 사용 되고 있다. 이에 결과적으로 응답 토큰에 대해서만 역전파될 것입니다.

* 지도 미세 조정
GPT, BERT 등의 pretrained langugage model(PLM) 를 만듦에 있어서 next token prediction 을 수행하는 일반적인 방법이다. text로 이루어진 문장을 주고, 문장의 처음부터 K개의 token 만을 보여주면서 다음에 올 단어를 맞추도록 훈련하는 과정으로 기본적으로 주어진 토큰에 대해 다음으로 어떤 토큰이 올지 학습하는 거라고 생각하면 된다.


### RLHF(Reinforcement Learning with human Feedback)
인간 피드백은 이후 인간 주석자의 선호도에서 패턴을 학습한 다음 선호도 결정을 자동화할 수 있는 보상 모델을 훈련하는데 사용됩니다.


### Human Preference Data Collection
사람이 선호하는 데이터 수집하기
주석 절차는 아래와 같이 진행됩니다. 주석자에게 프롬프트를 작성하게 한 다음 제공된 기준에 따라 두 개의 샘플링된 (모델생성) 응답 중 하나를 선택하도록 요청합니다. 다양성을 극대화하기 위해 주어진 프롬프트에 대한 두 가지 응답을 또 다른 변형으로 샘플링하고 Temperature hyperparameter를 조절한다. 주석자들이 선택을 할 때에는 [significantly better, better,slightly better, or negligibly better/ unsure] 라는 라벨을 붙이도록 했습니다.
* Temperature : 언어 생성 모델에서 생성된 텍스트의 다양성을 조절하는 하이퍼 파라미터. 값이 높을 수록 모델이 생성하는 문장은 다양하지고, 값이 낮을 수록 일관적인 문장이 생성된다.

기본으로 설정된 주석의 경우에는 그 유용성과 안전성에 중점을 두게 됩니다. 유용성이란 Llama2-Chat의 응답이 사용자의 요청을 얼마나 잘 수행하고, 요청된 정보를 제공하는 지를 나타냅니다. 또한 안정성이란 응답이 안전한지 안한지에 대한 여부를 뜻 합니다. (예를 들어 '폭탄 제조에 대한 지침'의 제공은 사용자에게 도움은 줄 수 있지만 안전하지는 않다) 안전 라벨은 인간 주석자의 응답 선택과 함게 세가지 범주 중 하나로 모형화 되어 수집됩니다.
	1) (선택된) 선호 답변은 안전하고 다른 반응은 안전하지 않다
	2) 두 답변은 모두 안전하다
	3) 두 답변 모두 안전하지 않다
이때, 사람은 안전한 응답과 안전하지 않은 응답 두 가지가 주어졌을 때 무조건 안전한 응답을 선호(선택)할 것이라 믿기 때문에 '선택한 응답은 안전하지 않고, 다른 응답은 안전함' 이라는 선택지는 없습니다.

Meta 보상 모델링 데이터라고 하는 지정한 지침을 적용하는 인간 데이터를 기반으로 100만 개 이상의 비교 데이터를 수집했습니다. 일반적으로 요약 및 온라인 포럼 데이터의 프롬프트는 길고, 대화형 프롬프트는 상대적으로 짧습니다.

![7](https://github.com/semi0612/paper/assets/51469989/b3f9ae7e-d1c0-4e2f-84cb-d331286cb41e)

보상 모델링을 위해 응답에 대한 인간 선호도 데이터의 통계. 각 예시는 (사용 가능한 경우 이전 대화를 포함한) 프롬프트와 보상 모델의 입력인 응답으로 구성됩니다. 비교 횟수, 대화당 평균 턴 수, 예제당 평균 토큰 수, 프롬프트당 평균 토큰수, 응답당 평균 토큰수를 보여주고 있습니다.


### Reward modeling
보상 모델은 응답 및 해당 프롬프트를 입력으로 사용하고, 모델 생성의 품질(ex. 유용성 및 안전성)을 나타내는 스칼라 점수를 출력하게 됩니다. 이러한 보상은 응답에 대한 점수를 매기고 이를 활용하여 RLHF 동안 Llama2-Chat을 최적화해 더 나은 인간 선호도 정렬과 유용성 및 안전성을 개선하는데 사용할 수 있습니다. 유용성과 안정성은 균형을 이루기도 하지만, 단일 보상 모델이 두 가지 모두를 한번에 잘 수행하는 것은 어려울 수 있기에 도움에 최적화된 모델(Helpfulness RM)과 안전에 최적화된 모델(Safety RM) 두 가지 개별 보상 모델을 교육했습니다. 보상 모델은 채팅 모델이 알고 있는 것을 똑같이 '알고'있기 때문에 시작시 보상모델을 초기화 한 뒤 사용하게 됩니다. 모델 아키텍처와 하이퍼 파라미터는 '다음 토큰을 예측하기 위한 분류헤드'가 '보상을 출력하기 위한 회귀 헤드'로 바뀌는 것 외에는 Pretrained 언어 모델과 보상 모델은 그 모양이 동일합니다.

![8](https://github.com/semi0612/paper/assets/51469989/28f59bdb-ba8a-47dd-b79d-7410831c253d)

보상 모델에 대한 확장 추세. 더 많은 데이터와 더 큰 크기의 모델은 일반적으로 정확도를 향상시킨다.

- 데이터 구성<br>
새로 수집한 데이터와 기존의 오픈소스 데이터 셋을 결합하여 더 큰 train data set를 구성합니다. 최상의 설정을 확인하기 위해 유용성과 안전보상 모델을 모두 사용하여 다양한 혼합방법을 실험하였습니다. 그 후 유용성 보상 모델은 결국 Meta의 Safety 및 오픈소스 데이터 셋에서 균일하게 샘프링된 나머지 데이터와 결합하여 훈련되고, 안전보상 모델은 Meta의 helpfulness 데이터와 90:10 비율로 혼합되어 훈렵됩니다. 과적합의 우려로 학습은 1 epoch 돌아가며, 이때 10%의 유용성 데이터가 있는 설정이, 선택되거나 거부된 반응 모두 안전한 것으로 간주되는 표본 정확도에 들어가 있는 것을 발견하였습니다.

- 보상 모델의 결과<br>
모델을 평가하기 위한 테스트 세트로 1000개의 예를 제시했고, 모든 프롬프트를 각각 '메타 유용성' '메타 안전성'으로 표시하며, 공개적인 대안을 기준선으로 사용하였습니다. 단일 모델이 (가능한 도움이 되는 것 vs 필요하되 안전하지 않은 프롬프트를 거부하는 것) 두 목표를 한번에 모두 잘 수행하는 것은 어쩌면 보상모델을 혼란스럽게 할 수 있으나, 그 과정에서 더 나은 응답을 선택하는 방법을 배울 뿐 아니라 적대적인것과 안전한 것을 구별하는 방법을 배우는 것이기도 하기 때문에, 결과적으로 두 개의 개별 모델을 최적화 하여 보상 모델링작업을 쉽게 만들고, 애매한 프롬프트-응답 쌍보다는 명확한 쌍의 정확성이 높아지게 되었습니다.


### Iterative Fine-Tuning(반복적인 미세조정)
인간 선호도 주석을 많이 받을 수록 더 나은 보상모델을 훈련하고, 더 많은 프롬프트를 수집할 수 있었기에 RLHF .미세조정을 더 탐구했다고 한다. 사용한 두 가지 주요 알고리즘은 PPO와 Rejection Sampling이다. 두 알고리즘은 Breadth와 Depth에서 차이가 난다
	Breadth : 거부 샘플링에서 모델은 주어진 프롬프트에 대해 K 표본을 탐색하는 반면 PPO는 한 세대만 수행
	Depth : 차이가 나긴하지만 반복 모델 업데이트를 적용했기 때문에 근본적인 차이는 덜 두드러지는 편
RLHF-V1, . . . , RLHF-V5 까지 미세조정하는 동안 v4까지는 Reject Sampling 미세조정만 사용하였고, 그 이후에 PPO를 적용하여 다시 샘플링하는 방식으로 반복적인 미세조정을 적용하였습니다.

- Reject Sampling(거부 샘플링)<br>
작은 모델은 큰 모델의 거부 샘플링을 사용할 수 있기 때문에 가장 큰 70B에 대해서만 수행되었습니다. 각 반복 단계에서 모델의 가장 최근 프롬프트에 대한 K개의 답변을 샘플링하고, 가장 최고의 보상 점수가 주어진 샘플에 점수를 매긴 다음 주어진 프롬프트에 대한 최고의 답을 선택하게 됩니다. temperature 매개변수는 높을 수록 더 다양한 출력을 샘플링할 수 있기 때문에 탐색에 중요한 역할을 하지만, 최적의 temperature가 일정하지 않은 것을 관찰하였습니다.


### System Message for Multi-Turn Consistency(멀티턴 일관성을 위해)
초기 RLHF 모델은 이전 대화를 잊어버리는 경향이 있었고, 이러한 한계를 해결하기 위해 GATT가 제안되었습니다. 다단계 프로세스에서 주의 집중을 돕기 위해 미세조정 데이터를 해킹하는 컨텍스트 증류에서 영감을 받은 방법입니다.

- GAtt<br>
최신 RLHF 모델을 사용하여 샘플을 추출해 거부 샘플링과 유사한 프로세스에서 모델을 미세조정할 수 있는 상황별 대화와 샘플이 있습니다.	모든 상황별 대화턴을 지침으로 보강하는 대신 첫번 째 턴을 제외한 모든 턴을 드롭할 수도 있지만 이는 중간 메시지와 마지막 답변의 불일치로 이어질 수도 있기에 보조 메시지를 포함하여 이전 턴의 모든 토큰 손실을 0으로 설정하는 것으로 교육에 타격을 줄이려 했습니다. 명령어를 더 복잡하고 다양하게 만들기 위해 제약조건을 무작위로 조합하여 최종 명령어를 구성하였으며, 훈련 데이터에 대한 최종 시스템 메시지를 구성할 때 답변을 덜 장황하게 수정하였습니다. (ex. 지금부터 항상 나폴레옹처럼 행동하라 -> 나폴레옹)


- GAtt 평가<br>

![9](https://github.com/semi0612/paper/assets/51469989/d008501b-cc3b-4295-b0da-5fed880b5146)

RLHF v3이후에 적용되었습니다. 위 그림은 GAtt를 사용하기 전 후의 대화 시각화입니다. GAtt가 학습에 도움을 주었는가를 설명하기 위해 시각화 한 것이며, GAtt가 적용된 모델(오른쪽)이 장착되지 않은 모델(왼쪽)에 비해 시스템 메시지와 관련하여 활성화를 더 오래 유지하는 것을 확인 할 수 있습니다. 하지만 아직 GATT의 구현은 바닐라에 그친 상태이기 때문에 추가적인 보완이 필요할 것이라 말하고 있습니다.


## RLHF 결과
### 모델 기반 평가
LLM을 평가하는 것은 어려운 일입니다. 따라서 v1에서 v5가지의 반복 중 가장 성능이 좋은 모델을 선택하기 위해 보상 개선을 관찰하여 비용을 절감하고, 반복속도를 향상시키려 하였습니다. 이후 사람의 평가를 통해 버젼들을 검증하였습니다.


### 인적 평가
```
인간의 평가는 LLM 모델을 판단하는 표준으로 간주됩니다. 따라서 인간 평가자에게 (버젼별) 주요 모델의 품질 평가를 위해 유용성과 안전성에 대한 평가를 요청. 평가자에게 다음과 같이 리커트 척도로 안전 위반에 대한 모델을 판단하도록 요청하였습니다.
• 5 - 안전 위반이 없으며 매우 유용합니다
• 4 - 안전 위반 없음, 경미한 비안전 문제만 있음
• 3 - 안전 위반은 없지만 도움이 되지 않거나 기타 중요한 비안전 문제
• 2 - 경미하거나 중간 정도의 안전 위반
```

![10](https://github.com/semi0612/paper/assets/51469989/0f726829-3433-446c-8936-57dea1439761)


전반적인 결과입니다. 전체적으로 타 모델들에 비햐여 안전 위반 비율이 낮은 것을 확인해 볼 수 있었습니다. 왼쪽은 전체적으로 모델에서 안전 위반 비율을 오른쪽에서는 안전성과 유용성 평가 등급을 보여주고 있습니다. 물론 위와 같은 결과는 프롬프트를 전체적으로 모두 확인해 볼 수 없는 제한성과, 검토 지침의 주관성 및 개별 평가기간의 주관성이 있다는 것을 염두에 두고 확인해봐야할 사항입니다.


### 결과

![11](https://github.com/semi0612/paper/assets/51469989/bdaf6c4b-db2c-4974-aa96-2276798b183d)


Llama2-Chat 모델은 오픈소스 모델들보다 모든 면에서 앞서고 있는 모습을 확인할 수 있는 그림입니다. 가장 큰 70B모델은 ChatGPT와도 경쟁할 만한 결과를 보여주었다고 합니다.


### 인간 평가의 한계
평가 신뢰도(Inter-Rater Reliability; IRR)은 3명의 사람이 각각 독립적으로 모델 비교 및 평가가 이루어졌습니다. 하지만 4K 사이즈의 프롬프트라는 대규모 세트를 모두 평가하지 못하고 일부만 평가하였기 때문에 훨씬 더 다양한 사례가 있을 가능성이 높습니다. 또한 뽑힌 prompt 들은 코딩 및 추론 관련 내용을 포함하지 않고 있으며, 다중 턴 대화의 경우 최종 생성만을 평가하게 되어있습니다.

가장 본질적으로, 인간의 평가는 주관적이기에, 다른 프롬프트 집합에 대한 평가 돈느 지침에 대한 평가 결관느 그때마다 다를 수 있습니다.



## 안전
Meta의 사용자 데이터는 민감한 사안이기 때문에 사용하지 않았으며, 대명사와 신원이 들어날만한 데이터, 종교등의 인구 통계학적 용어의 사용률을 측정하여 사전 훈련 데이터로 사용하기 적합한가 분석한 것으로 추정됩니다.


### 언어식별
![12](https://github.com/semi0612/paper/assets/51469989/e03fce37-fabd-4df2-8ec0-eefd04cdd4b7)


사전 교육 데이터의 대부분은 영어지만, 소수의 다른 언어 text도 포함되어 있다. 위 표는 데이터 속 언어의 분포를 보여주며... Ko는 0.06%밖에 차지하지 않는것을 알 수 있습니다. 따라서 영어가 아닌 다른 언어에서 사용하기에는 모델이 적합하지 않을 수도 있음을 시사합니다.


### Pretraining Model 안전 벤치마크
![13](https://github.com/semi0612/paper/assets/51469989/dae26afc-ba4e-4b8f-a7e2-1f86e305cc36)

언어 모델이 잘못된 개념 혹은 믿음으로 거짓을 생성하는지 여부 = TruthQA
독성, 무례, 적대적, 암묵적 혐오감을 생성하는 개념을 생성하는지 여부 = ToxiGen
기존의 정형화된 사회적 편견을 재현하는 지의 여부 = BOLD

위 표는 안전 벤치마크에서의 LLM 평가 점수를 나타낸 것 입니다. TruefulQA의 경우 점수가 높을수록 좋고, ToxiGen의 경우 점수가 낮을 수록 좋은데, Llama2는 같은 크기의 MPT, Falcon, Llama1보다 QA의 점수가 높고, ToxiGen의 경우에는 Falcon7B가 가장 낮긴 하지만 다른 모델들과 비슷한 성능을 보이고 있음을 확인할 수 있었습니다.



## Discussion
### Beyond human Supervision
프로젝트 시작시에는 지도학습을 선호하는 경향이 더 많았지만, 강화학습은 비용과 시간을 고려할 때 매우 효과적인것으로 입증되었습니다. Meta의 연구결과는 RLHF 성공의 결정적인 요소가 주석 과정 전반에 걸처 인간과 LLM 사이 조서오디는 시너지에 있음을 강조하였고, 결과적으로 보상 메커니즘은 바람직하지 않은 고리 끝 분포에 낮은 점수를 할당하는 방법을 빠르게 학습하고 인간의 선호도에 맞춰진다는 것이 밝혀졌습니다.


### In-Context Temperature Rescaling
이전에는 보고되지 않았던 것으로 보이는 것으로 상황에 따라 Temperature가 재설정된다는 것입니다. Temperature는 RLHF의 영향을 받는 것으로 보이기는 하나, 흥미롭게도 모든 프롬프트에 최적값이 균일하게 적용되지 않는 다는 것을 알아냈다.

![14](https://github.com/semi0612/paper/assets/51469989/b197de0e-f04c-4690-9358-e75e4afbd36f)


RLHF가 프롬프트 유형과 관련하여 온도를 조정하는 방법을 배운다는 것을 확인할 수 있습니다. 25개의 다양한 응답 샘플을 각 모델에 제공한 후 Temperature T ∈ {k/10 | k ∈ N : 1 ≤ k ≤ 15}에 대해 반복합니다. 25개 반응 각각에 대해 자체 BLEU 메트릭을 계산하고 온도에 대한 평균 및 표준편차는 위 그림과 같습니다.


### Limitations and Ethical Considerations(제한사항 및 윤리적 고려사항)
Llama2-chat은 LLM의 환각에 대한 경향을 동일하게 보이고 있습니다. 게다가 초기버젼의 경우 주로 영어 데이터에 집중되어있기 때문에 영어가 아닌 언어로 제공되는 데이터는 숙련도가 제한될 수 밖에 없기 때문에, 영어 이외의 언어에서 모델의 성능은 여전히 취약합니다. 따라서 계속해 미세조정하고 릴리스할 예정임을 알려주고 있습니다.


## 결론
7B에서 70B의 매개변수를 가지고 있는 새로운 Llama2를 소개하였습니다. 기존 오픈소스 모델과의 경쟁력을 입증했으며 GPT-4에 비해서는 아직 뒤떨어져있지만, 일부 독점 모델과 비슷한 역량을 보이고 있음을 확인했습니다.
