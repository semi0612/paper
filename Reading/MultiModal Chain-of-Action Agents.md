  > You Only Look at Screens: MultiModal Chain-of-Action Agents 논문
https://arxiv.org/pdf/2309.11436.pdf
  
	[ABSTRACT]
    최근 연구에서 다양한 환경에서 LLM의 기능을 이끌어내는 다양한 방법을 조사함
	기본적인 접근 방식은 샌드박스 환경에서 외구 도구와 애플리케이션별 API에 의존하여 환경을 텍스트 요소로 파싱하고 예측된 동작을 해석하는 방향으로 개발
	그러나 이런 접근 방식은 종종 추론의 비효율성과 오류 전파 위험(아마도 전송하는 과정에서의 오류를 말하는 듯)이 있음
	
	그런 문제를 완화하기 위해 환경 분석이나 애플리케이션별 API의 의존도를 우회하여, 인터페이스와 직접 상호 작용할 수 있는 멀티모달 솔루션인 Auto-UI를 소개
	거기에 더해서 이전 작업 이력과 이후 작업 계획을 활용하여 에이전트가 직접 실행할 작업을 결정하는데 도움이 될 수 있는, 연쇄 작업 기법을 제안한다.
	
	[INTRODUCTION]
	실제 상황에서 autonomous agents를 제한하는 두가지 주요 과제는 이렇다.
	1. 기존 접근 방식은 일반적으로 광학 문자 인식(OCR) 및 아이콘 감지기와 같은 외부 도구에 의존하여, 완경을 텍스트 요소로 언어모델의 입력하여 구문 분석 함.
	   그런데 이렇게 구문 분석이 된 요소는 긴 input 을 요구하기에 추론 효율성이 떨어질 수 있다.
	   추론 효율성이 떨어지면 계산 지연 시간이 생기거나, 언어 모델의 입력 길이 제한을 벗어날 수도 있음.
	   또한 시각적 환경을 텍스트로 분석하는 경우 실수가 생겨 오류 전파나 정보 손실이 발생할 수도 있음
	2. 기존 대부분의 접근 방식은 웹 페이지에서 자바스크립트 요소를 선택하거나(버튼 클릭같은거 말하는 듯), 파이썬 인터프리터를 사용하여 작업을 실행하는 등 환경과 상호작용하기 위해 내부 API에 접근해야하는 샌드박스 설정으로 이루어지고 있음
	   그러나 타사 앱에서 API 인터페이스에 접근할 수 없는 경우가 많음.
	   
	이런 점들이, 화면에 직접 상호 작용할 수 있는 보다 진보된 기술 개발에 동기 부여해줌. 그래서 인터페이스와 직접 상호 작용하는 멀티모달 접근 방식인 자동 UI 도입.
	에이전트의 행동 예측 능력을 향상 시키기위해 새로운 chain-of-action 기법을 제안함.
	여기서 말하는 Chain-of-Action이란, 행동 예측으로 이어지는 일련의 이전 행동 이력과 미래 행동 계획.
	
	작업은 다음과 같은 기술적 기여를 하고 있다고 요약할 수 있음.
	1. 화면과 직접 상호 작용할 수 있어 제약을 피할 수 있는 자율 UI 제어가 가능한, 멀티모달 에이전트인 Auto-UI를 소개
	2. 이전에 실행한 작업과 향후 작업 계획을 활용하여 에이전트가 각 단계에서 실행할 작업을 결정하는 데 도움을 주는 Chain-of-Action 기법 제안
	3. Auto-UI는 동작 유형 예측 정확도 90%, 실제 동작 성공률 74% 로 최첨단 성능을 구현했고, 특히 Auto-UI는 1초 이내의 빠른 동작 유추가 가능
	
	[RELATED WORK]
	이 작업은 language agents 분야에 속하기 때문에, 먼저 언어 에이전트 구축의 최근 상황을 언급한 다음, 언어 에이전트로 사용자 인터페이스를 제어하는 접근 방식에 대해 설명할 것.

	- 2.1 LANGUAGE AGENTS
	language agents는 사용자의 지시를 따르고 환경과 상호작용하여 작업을 완료할 수 있는 에이전트를 말한다.
	자율 에이전트(autonomous agents)와 대화형 에이전트(communicative agets) 두가지 유형이 널리 사용되는 중.
	자율 에이전트는 현실 세계에서 인간을 보조하는 것을 목표로한다. 대표적인 예로는 AutoGPT, BabyAGI, AgentGPT 등
	대화형 에이전트는 인간과 소통하고 협업할 수 있는, 개인화되고 사회화된 에이전트이다.
	사용자의 수동 개입 없이 앱 조작, 웹 쇼핑, 질문 답변 등 여러 단계의 작업을 완료할 수 있도록 지원하는 것을 목표로 한다.
	자연어로 된 사용자 지시가 주어지면 에이전트는 지시를 해석하고 사용자 인터페이스를 직접 제어하며 작업을 실행해야하는 것.
	
	- 2.2 UI CONTROL WITH NATURAL LAGUAGE
	최근 LLM은 instruction following 과 CoT(Chain-of-Thought) 프롬프트 기능을 갖춘 자율 UI에이전트 구축의 가능성을 보여주었다.
	특히 CoT prompt는 단계별 계획, 의사 결정(decision making), 행동실행(action execution)에 대한 LLM의 역량을 이끌어 냈다.
	그러나 작업환경은 LLM이 직접 처리할 수 있는 '자연어(natural language)'가 아닌 '그래픽 사용자 인터페이스(graphical user interfaces; GUIs;)' 임.
	따라서 GUI 상태와 동작을 LLM의 입력 및 출력 형식에 맞게 텍스트 형식으로 변환해야 한다.
	예를 들어 아이콘 인식과 OCR을 통해 UI 화면을 파싱하고, 파싱한 요소를 HTML 레이아웃으로 구성하는 것.
	기존 접근 방식은 샌드박스 환경에서 환경 구문 분석과 동작 해석을 위해 외부도구 및 앱 API 에 의존했기 때문에 효율적인 추론과 전파 오류에 어려움을 겪을 수 있음.
	멀티모달 아키텍처를 고려한 연구들도 있지만, 역시나 경쟁력있는 성능을 보장하기 위해 세분화된 환경 파싱에 의존하는 경향.
	본 연구는 추가적인 환경파싱 없이 UI를 직접 읽고, 별도의 API도 없이 실행할 수 있는 동작을 제공하는 제1원칙 사고에 기반하여 구축됨.
	
	[METHODELOGY]
	먼저 UI제어 작업에 대한 기본 개념을 소개한 다음, Auto-UI 프레임워크의 설계에 대해 설명한다
	- 3.1 PROBLEM FORMALIZATION.
		  사용자 지시(요구사항,  목표)가 주어지면 상담원은 여러 단계의 상호작용을 통해 작업을 완료해야함.
		  전체 프로세스를 '에피소드'라고 하며 일련의 '화면'으로 구성.
		  에피소드의 각 단계마다 agent에게 스크린샷이 제공되며, agent는 작업이 완료 될때까지 작업을 예측해야함
		  
	- 3.2 FRAMEWORK OVERVIEW
		  Auto-UI는 입력 스크린샷과 user instruction이 주어지면 어떤 작업을 수행해야할지 결정하는 Multimodal agent.
		  agent의 의사 결정 능력을 강화하기 위해 일련의 이전 작업 이력과 향후 작업 계획을 활용하여 작업을 예측하는 Chain-of-action 방식을 도입
		  
		  Auto-UI의 모델 아키텍처는 그림2에 있고, 크게 3단계로 구성된다.
		  1. vision 과 언어 입력, 모두에서 인코딩된 특징을 획득. 구체적으로는 vision입력 즉 스크린샷은 frozen(고정된?) vision인코더에 의해 인코딩된다.
		     언어입력은 목표와 이전 작업 이력 체인으로 구성. 각 이력은 {action 유형, 터치 포인트, lift 지점, 입력된 텍스트} 모양의 튜플이 포함된다.
		  2. 인코딩된 시각 및 언어 표현은 self-attention 모듈에 의해 통합된다.
		  3. 통합된 표현은 디코더에 공급되어 향후 작업 계획 chain 즉, 향후 단계에서 실행할 작업유형 을 생성한 다음 작업예측을 수행함.
		     위의 절차에서 chain-of-action은 입력 측의 '이전 액션 이력'과 출력 측의 '미래 액션 계획' 두 부분으로 구성된다.
	
	- 3.3 COORDINATE NORMALIZATION
		  대상 작업은 'ACTION 유형, 터치 포인트, lift 지점, 입력된 텍스트' 같은 네 가지 구성 요소로 구성되어 있음.
		  여기서는 '이중 포인트 제스처(dual-point gesture), type, go_back, go_home, enter, 상태완료(status_compleate)'의 여섯 가지 동작 유형을 생각해봅시다.
		  이중 포인트 제스처는 터치 포인트와 [y, x] 좌표가 있는 lift 포인트로 구성된다.
		  제스처 동작은 유연한 동작 공간을 보장하며 임의의 위치에서 클릭과 스크롤을 나타낼 수 있음.
		  클릭 또는 스크롤 동작을 표현하는데 고정밀 좌표가 필요하지 않다는 것을 관찰했으니, 정규화된 좌표값을 적용하여 수렴을 가속화하고 좌표의 모호성을 완화함.
		  
	[EXPERIMENTS]
	- 4.1 DATASET
	평가에는 AITW 벤치마크 데이터 셋을 사용.
	AITW 는 자연어 명령어, 스크린샷, 동작이 포함된 UI 제어를 위한 대규모 벤치마크 데이터 셋이다.
	350개 이상의 앱과 웹사이트에서 앱의 조작, 웹 검색, 웹 소핑 등 다양한 다단계 작업을 다루는 30만 개의 고유 명령어로 구성된 715,000개의 에피소드로 구성되어 있다.
	또한 이미지역시 범용성을 보장하기 위해 다양한 해상도의 다양한 기기 유형과 운영체제를 포괄한다.
	표1(데이터 셋 통계)에는 일반, 설치, 구글 앱스, 단일, 싱글 이 다섯가지 하위 집합이 있다. 각 집합은 에피소드 별로 '트레이닝' '밸런싱'으로 나뉜다.
	
		1. 일반(General)에는 타사 앱 및 웹사이트와의 상호작용, 질문 답변이 필요한 기타 작업 등이 포함
		2. 설치(install)에는 앱 설치 및 제거, 앱 로그인, 앱 로그인 지원(App login support)와 관련된 작업이 포함
		3. 구글 앱스(Google Apps)에는 Gmail, 캘린더, 사진, 설정 등 다양한 애플리 케이션을 조작하는 방법이 포함
		4. 단일(Single)은 이전작업(예: Instagram 열기, 홈피드이동, 게시물 보기)이 이미 완료된 작은 작업(예: 게시물에 조아요 누르기)이 포함되어 있음
		5. 웹 쇼핑(WebShopping)에는 전자상거래 웹사이트의 온라인 쇼핑과 관련된 작업(예: 품목 검색, 장바구니에 품목 추가, 장바구니 보기 등)이 포함되어 있다.
	
	- 4.4 구현 세부 정보
	      프레임 워크에서 소형(60M), 기본(200M), 대형(700M) 설정에서 인코더-디코더 아키텍처를 채택하고 있음.
	
	- 4.5 주요 결과
