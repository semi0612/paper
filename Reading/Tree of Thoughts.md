  Tree of Thoughts (ToT)
  언어 모델은 다양한 작업에서 일반적인 문제 해결에 사용되고 있지만, 추론 과정은 여전히 토큰 레벨에 갖힌 채로, 토큰을 하나씩 순차적으로 결정하는 방식(left-to-right decision-making processes)으로 제한되어 있음
  이는 탐색, 전략적 미래 ㅐ예측 또는 초기 결정이 중요한 작업에서 한계를 가질 수 있음을 의미
  
  이 프레임워크는 언어 모델을 위한 많이 사용되는 "생각의 연쇄(Chain-of-Though)" 접근법을 일반화하며, 문제 해결을 위한 중간 단계로 작용하는 일관된 텍스트 단위인 "생각"에 대한 탐색을 가능하게함.
  ToT를 통해 LMs는 다양한 추론 경로를 고려하고 선택의 자가평가를 통해 다음 행동 방향을 결정하거나, 전체적인 선택을 위해 전방 또는 후방으로 조망하거나 되돌아가는 등 의도적인 결정을 내릴 수 있게 되는것.
  이 접근법을 통해 언어모델은 신중한 추론 과정을 거쳐 문제를 해결하기 위한 중간 생각들이 문제를 해결해나가는 과정을 자체적으로 평가할 수 있게 됨
  언어모델이 생각을 생성하고 평가하는 능력은 탐색 알고리즘(예: 너비 우선 탐색과 깊이 우선 탐색(DFS))과 결합되어, 선제적 탐색과 백트래킹이 가능한 생각의 체계적인 탐색을 가능하게 함.
  
  ToT를 사용할 때, 다른 작업들은 후보의 수와 생각/단계의 수를 정의하는 것을 요구합니다.
  예를 들어, 논문에서 보여진 바와 같이, 24 게임(Game of 24)은 생각을 중간 방정식을 포함하는 3단계로 분해하는 수학적 추론 작업이 사용.
  (24 게임은 다음 그림에서도 알 수 있듯이, 4, 9, 10, 13이 주어지면 (10-4)x(13-9)와 같이 24를 만드는 방식을 찾는 게임
  
  24 게임 작업을 위해 ToT에서 너비 우선 탐색(BFS)를 수행하며 각 단계에서 최선의 b=5개의 후보를 유지하고, LM은 각 생각 후보를 "확실함/아마도/불가능"으로 평가를 요청받는다.
  저자들은 "목표는 올바른 부분적 해결책을 촉진하는 것입니다. 몇 번의 예견 시험만으로 판단할 수 있고, "너무 크거나 작은" 상식에 기반하여 불가능한 부분 해결책을 제거하고, 나머지는 '아마도'로 유지합니다.""라고 함.
  각 생각에 대한 값은 3번 샘플링. (이 과정이 그림2에있음)
  
  실험 결과, ToT는 비非단순한 계획이나 탐색을 필요로 하는 세 가지 새로운 작업인 "24 게임", "창의적인 글쓰기" 및 "미니 크로스워드"에서 언어 모델의 문제 해결 능력을 크게 향상시켰는데,
  예를 들어 24 게임에서 "생각의 연쇄"를 사용한 GPT-4은 문제의 4%만 해결했지만, 우리의 방법은 성공률이 74%에 이르렀습니다. (그림 3)
  
  -> 근데 추론능력이 늘어나는 만큼, 필요할 리소스가 많아져서 실용성 면에서는 최적화 필요할 것으로 예상
