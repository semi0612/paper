∙ 학습 데이터. 현재 대부분의 VLM은 웹의 자연 이미지로 구성된 LAION[32]과 같은 데이터 세트에 대해 사전 학습됩니다.
  그러나 GUI 이미지가 자연 이미지와 다른 분포를 보인다는 것을 알 수 있습니다. 따라서 우리는 대규모 주석이 달린 데이터 세트를 구축합니다.
  지속적인 사전 학습을 위해 GUI와 OCR에 대한 대규모 주석 데이터셋을 구축합니다.
∙ 고해상도 대 컴퓨팅.
  GUI에서는 작은 아이콘과 텍스트가 어디에나 있으며, 일반적으로 사용되는 224 × 224 해상도에서는 이를 인식하기 어렵습니다.
  그러나 입력 이미지의 해상도를 높이면 언어 모델에서 시퀀스 길이가 상당히 길어집니다. 예를 들어 패치 크기가 14인 경우 1120 × 1120 이미지는 6400개의 토큰으로 구성된 시퀀스에 해당하므로 과도한 학습 및 추론 연산이 필요합니다.
  이 문제를 해결하기 위해 적절한 계산 예산 내에서 해상도와 숨겨진 크기 사이의 균형을 맞출 수 있는 교차 주의 분기를 설계합니다.
  구체적으로, 우리는 시각적 특징을 공동으로 모델링하기 위해 CogVLM[12](4.4B 파라미터)에 사용되는 기존의 대형 ViT[38]와 새로운 소형 고해상도 교차 모듈(0.30B 파라미터의 이미지 인코더 포함)을 결합할 것을 제안합니다.

실험을 통해 확인

∙ 코그에이전트는 AITW [31] 및 Mind2Web [10]을 포함한 유명 GUI 이해 및 의사 결정 벤치마크에서 1위를 차지했습니다. 우리가 아는 한, 일반 VLM이 추출된 구조화된 텍스트로 LLM 기반 방법을 능가하는 것은 이번이 처음입니다.
∙ CogAgent는 GUI에 초점을 맞추고 있지만, VQAv2 [1], OK-VQA [23], TextVQA [34], ST-VQA [4], ChartQA [24], infoVQA [26], DocVQA [25], MM-Vet [41], POPE [19] 등 9개의 시각적 질문 답변 벤치마크에서 최첨단 범용 성능을 달성했습니다.
∙ 예를 들어, 1120 × 1120 입력이 가능한 CogAgent-18B의 부동 소수점 연산(FLOP) 수는 기본 490 × 490 입력이 가능한 CogVLM-17B의 절반 이하로, CogAgent의 고해상도 및 저해상도 브랜치를 분리 설계하여 고해상도 이미지 소비에 드는 컴퓨팅 비용을 크게 낮췄습니다.
  CogAgent는 https://github.com/THUDM/CogVLM 에서 오픈 소스입니다. 이는 고급 VLM을 통해 AI 에이전트의 미래 연구 및 적용을 촉진하기 위한 노력의 일환입니다.
  
2. Method
이 섹션에서는 먼저 코그에이전트의 아키텍처, 특히 새로운 고해상도 크로스 모듈을 소개한 다음 사전 학습 및 정렬 과정을 자세히 설명합니다.

2.1. 아키텍처
코그에이전트의 아키텍처는 그림 2에 나와 있습니다. 우리는 사전 학습된 VLM(이미지 오른쪽)을 기반으로 모델을 구축하고, 고해상도 입력을 처리하기 위해 크로스 어텐션 모듈을 추가할 것을 제안합니다(이미지 왼쪽).
기본 VLM으로 오픈 소스 기반의 최신 대형 시각 언어 모델인 CogVLM17B [38]를 선택합니다.
특히 저해상도 이미지(224×224픽셀)를 위한 인코더로 EVA2-CLIPE [35]를 사용하고, 그 출력을 시각 언어 디코더의 특징 공간에 매핑하는 MLP 어댑터로 보완합니다.
사전 학습된 언어 모델인 디코더는 왕 외[38]가 소개한 시각 전문가 모듈로 강화되어 시각적 특징과 언어 특징의 심층적인 융합을 촉진합니다.
디코더는 저해상도 이미지 특징 시퀀스와 텍스트 특징 시퀀스의 결합된 입력을 처리하고 목표 텍스트를 자동 회귀적으로 출력합니다.
대부분의 VLM과 마찬가지로, 원래의 CogVLM은 다음과 같이 상대적으로 낮은 해상도(224 또는 490)의 이미지만 수용할 수 있는데, 이는 컴퓨터나 스마트폰의 화면 해상도가 컴퓨터 또는 스마트폰의 화면 해상도는 일반적으로 다음과 같습니다. 720p(1280 × 720픽셀) 이상입니다.
이는 VLM의 공통적인 문제입니다. 예를 들어 LLaVA [21] 및 PALI-X [8]는 다음과 같습니다. 일반 영역에서 224 × 224의 저해상도로 사전 학습됨.
도메인에서 사전 훈련됩니다.
가장 큰 이유는 고해상도 이미지 는 엄청난 시간과 메모리 오버헤드를 초래하기 때문입니다: VLM은 일반적으로 텍스트와 이미지 특징 시퀀스를 입력으로 연결합니다. 따라서 자체 주의 모듈의 오버헤드는 의 오버헤드는 시각적 토큰(패치)의 수에 이차적이므로 는 이미지의 측면 길이에 이차적입니다.
고해상도 이미지의 비용을 줄이기 위한 몇 가지 초기 시도가 있습니다.
예를 들어 예를 들어, Qwen-VL[2]은 이미지 특징을 압축하기 위해 위치 인식 비전 언어 어댑터를 제안하지만, 시퀀스 길이를 4개만 줄이고 최대 해상도는 448 × 448입니다.
코스모스-2.5[22]는 이미지 시퀀스의 길이를 줄이기 위해 인식기 리샘플러 모듈을 채택했습니다.
그러나 리샘플링된 시퀀스는 여전히 대용량 시각 언어 디코더(2,048개의 토큰)에서 자체적으로 처리하기에는 길며, 제한된 텍스트 인식 작업에만 적용될 수 있습니다.
따라서 우리는 고해상도 이미지에 대한 효율성을 유지할 뿐만 아니라 다양한 시각 언어 모델 아키텍처에 유연한 적응성을 제공하는 새로운 고해상도 크로스 모듈을 고해상도 이해도 향상을 위한 기존 구조의 강력한 보완책으로 제안합니다. 다양한 시각 언어 모델 아키텍처에 유연하게 적용할 수 있습니다.
2.2. 고해상도 크로스 모듈
고해상도 크로스 모듈의 구조적 설계는 주로 다음과 같은 관찰을 기반으로 합니다:

그림 2. CogAgent의 모델 아키텍처. 우리는 *CogVLM을 채택했습니다.
1. 224 × 224와 같은 적당한 해상도에서는 이미지가 대부분의 오브젝트와 레이아웃을 효과적으로 표현할 수 있지만 텍스트를 선명하게 렌더링하기에는 해상도가 부족합니다.
따라서 새로운 고해상도 모듈은 다음과 같이 GUI를 이해하는 데 필수적인 텍스트 관련 기능을 강조합니다.
2. 일반적인 영역에서 사전 학습된 VLM은 종종 큰 숨겨진 크기(예: PALI-X의 경우 4,096 및 CogVLM, LLaVA의 5,120), 문서 OCR과 같은 텍스트 중심 작업에 맞게 조정된 VLM은 만족스러운 성능을 달성하기 위해 더 작은 숨겨진 크기(예: 1,536 에서 1,536).
이는 다음을 시사합니다. 텍스트 관련 기능을 효과적으로 캡처하려면 더 작은 숨겨진 크기.
그림 2에서 볼 수 있듯이, 고해상도 크로스 모듈은 은 고해상도 입력을 위한 새로운 분기 역할을 합니다. 구현에서는 1120 × 1120 픽셀 크기의 이미지를 받아들입니다.
원래의 저해상도 입력 분기와 달리 고해상도 교차 모듈은 훨씬 더 작은 사전 학습된 비전 인코더(우리가 구현한 EVA2-CLIP-L [35]의 시각 인코더, 0.30B 파라미터)를 채택하고 작은 숨겨진 크기의 교차주의를 사용하여 고해상도 이미지 특징을 VLLM 디코더의 모든 계층과 융합하여 계산 비용을 줄입니다.
구체적으로, 입력 이미지의 경우 1120×1120 및 224×224로 크기를 조정하여 고해상도 크로스 모듈과 저해상도 분기에 각각 공급한 다음 서로 다른 크기의 두 이미지 인코더를 병렬로 사용하여 이미지 특징 시퀀스 Xhi 및 Xlo로 인코딩합니다.
시각 언어 디코더는 원래의 계산을 유지하며, 모든 디코더 레이어에서 Xhi와 숨겨진 상태 간의 교차주의를 통합하는 것만 변경됩니다.
공식적으로, 디코더의 i 번째 주의 계층의 입력 숨겨진 상태는 Xini ∈ R B×(LIlo+LT)×Ddec 이고, 크로스모듈의 이미지 인코더의 출력 숨겨진 상태는 Xhi ∈ R B×(LIhi)×Dhi 라고 가정합니다, 여기서 B는 배치 크기, LIlo , LIhi 및 LT는 저해상도 이미지, 고해상도 이미지 및 텍스트 시퀀스의 길이, Ddec 및 Dhi는 각각 디코더와 고해상도 인코더의 출력의 숨겨진 크기입니다. 각 계층의
주의 절차는 다음과 같이 공식화할 수 있습니다. + Xini , (1) Xouti = MCA(레이어노름(X′ i ), Xhi) + X′ i , (2) 여기서 MSA와 MCA는 시각 전문가와 멀티 헤드 교차 주의가 있는 멀티 헤드 자체 주의를 나타내고, X′ i와 Xouti는 잔여 연결이 있는 각각의 출력 특징을 나타냅니다. 이들 간의 교차 주의를 구현하기 위해 학습 가능한 변환 행렬 Wi Kcross , Wi Vcross ∈ RDhi×Dcross 를 추가하여 Ki cross = XhiWi Kcross 를 구합니다, V i 크로스 = XhiWi V 크로스 ∈ R LIhi×D 크로스 , 그리고 Wi Q 크로스 ∈ R Ddec×D 크로스를 추가하여 모든 디코더 레이어에서 Qi 크로스 = X′ iWi Q 크로스 ∈ R (LIlo+LT)×D 크로스를 얻습니다.
방정식 2의 잔여 연결을 사용하면 고해상도 이미지와의 교차 주의가 저해상도 이미지의 특징을 보완하는 것으로 인식할 수 있으므로 저해상도에서 이전에 사전 학습된 모델을 효과적으로 활용할 수 있습니다.
계산 복잡성. 교차 주의와 자기 주의에서 주의 헤드의 수를 Hcross와 Hdec로 하고, 각 헤드의 크기를 dcross = Dcross/Hcross, ddec = Ddec/Hdec로 하자. 고해상도 크로스 모듈을 사용하는 경우 주의의 계산 복잡도는 개선 = O입니다.

2.3 Pre-training
모든 사전 훈련 데이터는 공개적으로 사용 가능한 데이터 세트에서 파생되며, 사전 학습 데이터를 세 부분으로 나눈다. 
텍스트 인식 : 다양한 글꼴, 크기, 색상 및 방향의 텍스트와 LAION-2B의 다양한 이미지 배경이 포함되어 있음.
		  자연 이미지의 OCR. COYO 및 LAION-2B에서 자연 이미지를 수집하고 Padle-OCR 을 사용하여 텍스트와 그  경계상자를 추출하고 텍스트 상자가 없는 이미지를 필터링 한다.
		  침식, 가우시안 노이즈, 가우시안 블러, 이미지 압축, 탄성 변환등을 포함하는 Nougat와 동일한 데이터 증강을 적용시켜 텍스트 인식에 대한 모델의 강인함 향상
시각적 접기 : GUI 에이전트는 이미지 내의 다양한 요소를 정확하게 이해하고 위치를 파악할 수 있는 능력을 갖춰야 한다.
		  이미지-캡션 쌍으로 구성된 4000만장의 이미지로 구성된 시각적 데이터 세트를 사용. 바운딩 박스의 위치와 형식은 [[x, y, x, y]]이다.

batch size 4608, learning rate 2e-5, 6만회 반복
처음 2만번 동안은 새로 추가된 고해상도 교차 모듈을 제외한 모든 파라미터를 고정하여 646만개(3.5%)의 훈련 가능한 파라미터를 확보하고, 다음 4만회 동안은 CogVLM에서 시각적 전문가를 추가로 고정해제

2.4. Multi-task Fine-tuning and Alignment
휴태폰과 컴퓨터에서 2천개가 넘는 스크린샷을 수동으로 수집.
각 스크린샷에는 화면요소, 잠재적 작업, 작업 방법을 사람이 질문-답변 형식으로 기록









* CogVLM은 시각적 기능과 언어 기능의 긴밀한 통합으로 유명한 새로운 오픈 소스 시각적 언어 모델
주요 구성요소 : Vit 인코더 특히 EVA2-CLIP-E는 이미지 기능을 텍스트 기능과 더 잘 정렬하기 위해 최종 레이어가 제거되었음
		   MLP 어댑터는 이러한 이미지 기능을 텍스트 기능 공간에 매핑
		   GPT 스타일의 모델(CogVLM-17B의 Vicuna-7Bv1.5)이 될 수 있는 언어모델 구성 요소는 주의 작업에 인과 마스크를 적용
		   시각적 전문가 모듈은 QKV 매트릭스와 MLP로 구성된 모델의 각 계층에 추가되어 의미적 측면과 일치하도록 캡쳐한 이미지 특징을 변환하여 시각적 언어 특징을 심층적으로 정렬
		   이는 시각적 기능과 언어 기능의 정교한 융합을 가능하게 하여 크로스 모달 벤치마크에서 모델의 최첨단 성능을 촉진한다
